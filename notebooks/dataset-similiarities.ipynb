{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook calculates the average similarity between the embeddings of the three different datasets - the original TruthfulQA misconceptions dataset, the crafted, and generated. Optionally, this comparison also includes the provided answer options in addition to the dataset questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to include the answers to questions when calculating similarity.\n",
    "EMBED_QUESTION_ANSWERS: bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard to handle notebooks being stored in a subdirectory\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from truthfulqa_dataset import load_truthfulqa\n",
    "import datasets\n",
    "import numpy as np\n",
    "import prettytable\n",
    "import sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = sentence_transformers.SentenceTransformer(\n",
    "    \"all-mpnet-base-v2\", device=\"cpu\"\n",
    ")\n",
    "\n",
    "\n",
    "def embed(text: str) -> np.array:\n",
    "    return embedding_model.encode(text, convert_to_tensor=True).numpy()\n",
    "\n",
    "\n",
    "def get_truthfulqa_dataset_embeddings(\n",
    "    truthfulqa_dataset: datasets.Dataset,\n",
    "    exclude_choices: bool = False,\n",
    ") -> np.array:\n",
    "    \"\"\"\n",
    "    Embed elements from a dataset that uses the TruthfulQA structure.\n",
    "\n",
    "    Args:\n",
    "        truthfulqa_dataset (datasets.Dataset): The dataset to embed.\n",
    "        exclude_choices (bool, optional): If this is True, only the\n",
    "            questions will be embedded. If this is False, the questions\n",
    "            and choices will be embedded. Defaults to False.\n",
    "    \"\"\"\n",
    "    if exclude_choices:\n",
    "        texts = truthfulqa_dataset[\"question\"]\n",
    "    else:\n",
    "        texts = [\n",
    "            \"\\n\".join([x[\"question\"]] + sorted(x[\"mc1_targets\"][\"choices\"]))\n",
    "            for x in truthfulqa_dataset\n",
    "        ]\n",
    "    return embedding_model.encode(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8227]])\n",
      "tensor([[0.2015]])\n",
      "tensor([[0.1572]])\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "s1 = \"What is the capital of France?\"\n",
    "s2 = \"Which is the largest city of the French nation?\"\n",
    "s3 = \"When did mankind first step foot on the moon?\"\n",
    "\n",
    "e1 = embed(s1)\n",
    "e2 = embed(s2)\n",
    "e3 = embed(s3)\n",
    "\n",
    "print(sentence_transformers.util.pytorch_cos_sim(e1, e2))\n",
    "print(sentence_transformers.util.pytorch_cos_sim(e1, e3))\n",
    "print(sentence_transformers.util.pytorch_cos_sim(e2, e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nondiag_mean(arr2d: np.array) -> float:\n",
    "    return arr2d[~np.eye(arr2d.shape[0], dtype=bool)].mean()\n",
    "\n",
    "\n",
    "def self_similarity(embs: np.array) -> float:\n",
    "    sims = sentence_transformers.util.cos_sim(embs, embs).numpy()\n",
    "    return nondiag_mean(sims)\n",
    "\n",
    "\n",
    "def cross_similarity(embs1: np.array, embs2: np.array) -> float:\n",
    "    sims = sentence_transformers.util.cos_sim(embs1, embs2).numpy()\n",
    "    return np.mean(sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes [(100, 3), (24, 2), (99, 3)]\n"
     ]
    }
   ],
   "source": [
    "# 1. Load datasets\n",
    "# @TODO Make utilities for these.\n",
    "\n",
    "truthful_dataset = load_truthfulqa(\"misconceptions\")\n",
    "crafted_ds = datasets.load_dataset(\n",
    "    \"json\", data_files=\"../datasets/crafted_dataset_unfiltered.jsonl\"\n",
    ")[\"train\"]\n",
    "generated_ds = datasets.load_dataset(\n",
    "    \"csv\", data_files=\"../datasets/generated_dataset_unfiltered.csv\"\n",
    ")[\"train\"]\n",
    "\n",
    "\n",
    "def array(x, dtype=None):\n",
    "    return x\n",
    "\n",
    "\n",
    "# Special logic due to how the CSV stores choices as a string\n",
    "generated_ds = generated_ds.map(\n",
    "    lambda x: {\n",
    "        \"question\": x[\"question\"],\n",
    "        \"mc1_targets\": eval(x[\"mc1_targets\"], dict(globals(), array=array), locals()),\n",
    "    }\n",
    ")\n",
    "\n",
    "dss = [truthful_dataset, crafted_ds, generated_ds]\n",
    "dss_names = [\"Orig\", \"Craft\", \"Gen\"]\n",
    "\n",
    "print(\"Dataset shapes\", [ds.shape for ds in dss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100, 768), (24, 768), (99, 768)]\n"
     ]
    }
   ],
   "source": [
    "# 2. Embed each item\n",
    "\n",
    "all_embeddings = [get_truthfulqa_dataset_embeddings(ds) for ds in dss]\n",
    "\n",
    "print([embs.shape for embs in all_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Calculate similarities\n",
    "\n",
    "self_similiarites = [self_similarity(embs) for embs in all_embeddings]\n",
    "cross_similiarities = [\n",
    "    [\n",
    "        self_similarity(embs1) if embs1 is embs2 else cross_similarity(embs1, embs2)\n",
    "        for embs2 in all_embeddings\n",
    "    ]\n",
    "    for embs1 in all_embeddings\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross similiarities:\n",
      "+-------+------------+-------------+------------+\n",
      "|       |    Orig    |    Craft    |    Gen     |\n",
      "+-------+------------+-------------+------------+\n",
      "|  Orig | 0.10230102 | 0.082592644 | 0.09238595 |\n",
      "| Craft | 0.08259264 | 0.097121224 | 0.08119775 |\n",
      "|  Gen  | 0.09238594 |  0.08119775 | 0.11531446 |\n",
      "+-------+------------+-------------+------------+\n"
     ]
    }
   ],
   "source": [
    "# 4. Print results\n",
    "\n",
    "print(\"Cross similiarities:\")\n",
    "tmp = prettytable.PrettyTable(field_names=[\"\"] + dss_names)\n",
    "tmp.add_rows(\n",
    "    [[dss_names[i]] + list(cross_similiarities[i]) for i in range(len(dss_names))]\n",
    ")\n",
    "print(tmp.get_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrospective-llm-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
